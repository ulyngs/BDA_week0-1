---
title: "Big Data Analytics Week 1"
author: "Ulrik Lyngs (based on Bertie Vidgen's materials)"
date: "Last updated: 9 March 2020"
output: html_document
---

# BDA Week 1 notes
This week we look at how to quantify the spread of a distribution using the Gini coefficient and shannon entropy. We will also plot a histogram and calculate the mode. We provide examples with the 'mtcars' dataset (comes with R) and some sample wikipedia data.

Set up the R workspace and load the required packages/data.

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
options(scipen = 10) #makes the output more readable by increasing the number of values before scientific notation is used.

fun_check_packages <- function(x){
  for (i in seq(1,length(x))){
    if (!x[i] %in% installed.packages()){
    install.packages(x[i])}
    library(x[i],character.only=TRUE)}
}
packs = c('ineq','ggplot2', 'entropy', 'dplyr')
fun_check_packages(packs)

```

### Data
In the code below, we calculate the gini coefficient and then plot the lorenz curve for the 'mtcars' dataset. This dataset comes with every installation of R. You can apply the code to the data you've collected from Wikipedia.

We are going to look at the 'disp' variable in the dataset 'mtcars', which has data for 32 cars from 1973-74 on various variables and is built in to R. 'disp' stands for displacement which says something about how the cars' engines are structured. 

```{r}
mtcars
```


### Histogram
First, make a histogram to show the distribution of the variable:

```{r hist}
ggplot(mtcars,
       aes(disp)) +
  geom_histogram(bins = 10)

```

### Mode
Second, calculate the mode of the variable, and how many values are equal to it.
Unfortunately, R does not have an in-built mode function, so we have to create one:

```{r mode}
getMode <- function(x, na.rm = FALSE) {
  if(na.rm){
    x = x[!is.na(x)]
  }
  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
} # from https://stackoverflow.com/questions/2547402/is-there-a-built-in-function-for-finding-the-mode

getMode(mtcars$disp) #mode

# How many of the values equal the mode?
numMode <- length(mtcars$disp[mtcars$disp == getMode(mtcars$disp)])

print(numMode)

```

Here's an alternative approach that might be more transparent.
The '%>%' operator is from the `dplyr` package and simply means 'then do this'.

```{r}
# take the mtcars dataset, count number of rows for each entry in 'disp'
mtcars %>% 
  count(disp) %>% 
  arrange( desc(n) )

```

It is clear that the most frequently value is 275.8, which occurs 3 times.


### Gini coefficient
Third, calculate the gini coefficient and plot the Lorenz curve.

The Gini coefficient is a measure of statistical dispersion. It can be applied to any variable to see how it is distributed - most famously, it has been used to measure income distribution. A score of '1' indicates perfect inequality. In the case of income, this means that one person has all the income and no-one else has any. A score of '0' indicates perfect equality. In the case of income, this means that every person has the exact same income.

The Gini coefficient is the area between the line of 'perfect equality' (where the gini coefficient is 0; the distribution of the variable is uniform, i.e. totally equal) and the 'lorenz curve' (which is the actual distribution of values across the population).

Sal Khan (the Khan Academy founder) has a crystal clear explanation of the Gini coefficient and the Lorenz curve here: https://www.youtube.com/watch?v=y8y-gaNbe4U

We can calculate the gini coefficient using the `ineq` function from the `ineq` package, and plot the Lorenz curve with base R:

```{r gini}
ineq::ineq(mtcars$disp,
           type='Gini') #calculate the gini coefficient for the variable disp

```

The `Lc` function from the `ineq` package calculates values for the Lorenz curve. 
We plot the Lorenz curve with base R like this:

```{r}
plot(ineq::Lc(mtcars$disp),
     col = 'red',
     lwd = 2)
```


Or with ggplot like this:
```{r}
lorenz_data <- ineq::Lc(mtcars$disp)

# ggplot needs your data to be a data frame object
lorenz_data_frame <- data.frame(p = lorenz_data$p, 
                                L = lorenz_data$L)

ggplot(lorenz_data_frame, aes(p, L)) +
  geom_segment(aes(x = 0,
                   y = 0,
                   xend = 1,
                   yend = 1)) +
  geom_line(color = 'red') +
  labs(x = "% population", y = "% variable")
# based on: https://stackoverflow.com/questions/22679493/how-to-plot-a-nice-lorenz-curve-for-factors-in-r-ggplot

```


If you find the Gini coefficient particularly interesting, then have a look at some additional material here: https://www.r-bloggers.com/sampling-distribution-of-gini-coefficient/


### Shannon entropy
Entropy is a measure of disorder within a distribution; higher entropy means that a system is more disordered and therefore harder to predict. If you want a refresher, with a nice worked through example, I found a great Youtube video here - https://www.youtube.com/watch?v=ErfnhcEV1O8&list=LLpnMwTVTiriukuWO3AenRSA&index=2&t=333s 
Only the first few minutes are relevant for understanding Shannon entropy (they guy then goes onto discuss cross-entropy) but it really is worth a look.

We can use the `entropy` package to calculate entropy in some data.

Let's work through an example with the distribution of number of gears in the mtcars dataset:

```{r shannon-entropy}
library(entropy)

# take a look at the values in the 'gears' variable in the mtcars dataset
mtcars$gear

#the entropy function in the entropy package estimates the Shannon entropy in a variable, if we give it the observed counts. Therefore, we first count how often each type of gear occurs
gear_frequencies <- mtcars %>% 
  count(gear) %>% 
  mutate(equal_counts = 1) # the maximum entropy is when each type of gear is equally likely to occur - to calculate this, we also add a column where the count is '1' for all values

#have a look at the data frame
gear_frequencies

# calculate the actual entropy, by giving the column with counts to the entropy function - set the unit to 'log2' to get the entropy in bits
ent_actual <- entropy(gear_frequencies$n, unit = "log2")

# calculate the max entropy by giving it the column with equal counts
ent_max <- entropy(gear_frequencies$equal_counts, unit = "log2")
  
# the normalized entropy is the proportion of actual entropy to the maximum possible
ent_norm <- ent_actual / ent_max

```

The entropy for gears in the mtcars dataset is `r ent_actual`, the maximum entropy is `r ent_max`, and the normalised entropy is `r ent_norm`.


### Example with wikipedia data
Here is an example which uses wikipedia data that came from this query on [Quarry](https://quarry.wmflabs.org):

USE simplewiki_p;
SELECT * FROM page LIMIT 100;

Let's read in the data
```{r}
page_query <- read.csv("2020-02-26_page_query.csv")
```


Draw a histogram:

```{r}
ggplot(page_query, aes(page_len)) +
  geom_histogram()
```

Now let's draw the lorenz curve:

```{r}
lorenz_data <- ineq::Lc(page_query$page_len)

# ggplot needs your data to be a data frame object
lorenz_data_frame <- data.frame(p = lorenz_data$p, L = lorenz_data$L)

ggplot(lorenz_data_frame, aes(p, L)) +
  geom_segment(aes(x = 0,
                   y = 0,
                   xend = 1,
                   yend = 1)) +
  geom_line(color = 'red') +
  labs(x = "% population", y = "% variable")
# based on: https://stackoverflow.com/questions/22679493/how-to-plot-a-nice-lorenz-curve-for-factors-in-r-ggplot

```

Let's calculate the entropy for page length

```{r}
# create a dataframe with counts
page_length_frequencies <- page_query %>% 
  count(page_len) %>% 
  mutate(equal_counts = 1)

page_length_frequencies

# calculate actual entropy
ent_actual <- entropy(page_length_frequencies$n, unit = "log2")

# calculate max entropy
ent_max <- entropy(page_length_frequencies$equal_counts, unit = "log2")

# calculate normalised entropy
ent_norm <- ent_actual / ent_max

```

In this case, the entropy is `r ent_actual`, max entropy is `r ent_max` and normalised entropy is `r ent_norm`.

Why is the entropy so high? Because we treat each pagelength as a unique value - and the same exact page length is unlikely to occur multiple times.

Another approach is to bin our data in some way - just like a histogram does. We do this in the example below - try to play around with the number of bins and see how it affects the entropy!

```{r}
page_length_frequencies_binned <- page_query %>% 
  mutate(bin = cut_interval(page_len, 30)) %>%  #cut into 30 equally spaced bins
  count(bin, .drop = FALSE) %>%  #count how many are in each (.drop = FALSE means that we keep the values that are zero)
  mutate(equal_counts = 1)

# have a look
page_length_frequencies_binned

# calculate actual entropy
ent_actual <- entropy(page_length_frequencies_binned$n, unit = "log2")

# calculate max entropy
ent_max <- entropy(page_length_frequencies_binned$equal_counts, unit = "log2")

# calculate normalised entropy
ent_norm <- ent_actual / ent_max

```

Now, the entropy is `r ent_actual`, max entropy is `r ent_max` and normalised entropy is `r ent_norm`.



*** END ***




